# Qualifying Autonomous Systems: Learning from Mario Kart for Robust Safety

Kristy Sakano (Developer)  
Joe Mockler  
Alexis Chen (Developer)  

Fall 2024  


**Reward shaping** is a technique used in reinforcement learning (RL) to modify or "shape" the reward signal that an agent receives during training. The goal is to make learning more efficient by providing additional feedback that guides the agent toward desired behavior.  
  
The agent is given intermediate rewards that are related to the task's ultimate goals. These rewards are designed to encourage the agent to take actions that are expected to lead to better outcomes in the long run.
